<body class="body_style">
    <section id="Main-Part">
        <div class="container custom-container-width">
            <br>	

            <div class = "row">	
                <div class="col-md-12">
                    <h3 class="subtitle">Accepted Papers</h3>	
                    <hr class="m-y-2">	

                    <div style="margin-top: 1em;">
                        <h5>
                            <strong>Learning Scene-Level Signed Directional Distance Function for Aerial Autonomy</strong>
                            
                        </h5>
                        Zhirui Dai, Hojoon Shin, Yulun Tian, Ki Myung Brian Lee, Nikolay Atanasov
                        <br>
                        <strong>Abstract:</strong>
                            Dense differentiable environment representations
                            are critical for navigation and exploration by aerial robots. In this
                            work, we explore a novel implicit scene representation, the Signed
                            Directional Distance Function (SDDF), to enhance geometry mod-
                            eling and differentiable trajectory optimization. Unlike signed
                            distance function (SDF) and similar to neural radiance fields
                            (NeRF), SDDF has a position and viewing direction as input. Like
                            SDF and unlike NeRF, SDDF directly provides distance to the
                            observed surface along the viewing direction, allowing efficient
                            view synthesis without iterative ray marching. To learn and
                            predict scene-level SDDF efficiently, we develop a differentiable
                            hybrid representation that combines explicit ellipsoid priors and
                            implicit neural residuals. This approach allows the model to
                            effectively handle large distance discontinuities around obstacle
                            boundaries while preserving the ability for dense high-fidelity
                            prediction. We show that SDDF is competitive with the state-of-
                            the-art neural implicit scene models in terms of reconstruction
                            accuracy and rendering efficiency, while allowing differentiable
                            view prediction for robot trajectory optimization.
                        <br>
                        <a href="../attend/papers/13_SDDF.pdf">[paper link]</a>
                    </div>


                    <div style="margin-top: 1em;">
                        <h5>
                            <strong>Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis</strong>
                            
                        </h5>
                        Katherine Mao, Hongzhan Yu, Ruipeng Zhang, Igor Spasojevic, M Ani Hsieh, Sicun Gao, Vijay Kumar
                        <br>
                        <strong>Abstract:</strong>
                            Time-optimal trajectories drive quadrotors to their
                            dynamic limits, but computing such trajectories involves solving
                            non-convex problems via iterative nonlinear optimization, making
                            them prohibitively costly for real-time applications. In this work,
                            we investigate learning-based models that imitate an model-
                            based time-optimal trajectory planner to accelerate trajectory
                            generation. Given a dataset of collision-free geometric paths,
                            we show that modeling architectures can effectively learn the
                            patterns underlying time-optimal trajectories. We introduce a
                            quantitative framework to analyze local analytic properties of
                            the learned models, and link them to the Backward Reachable
                            Tube of the geometric tracking controller. To enhance robustness,
                            we propose a data augmentation scheme that applies random
                            perturbations to the input paths. Compared to classical planners,
                            our method achieves substantial speedups, and we validate its
                            real-time feasibility on a hardware quadrotor platform.
                        <br>
                        <a href="../attend/papers/12_Implicit_Model.pdf">[paper link]</a>
                    </div>

                    

                    <div style="margin-top: 1em;">
                        <h5>
                            <strong>Semi-Implicit Data-Driven Predictive Control for Agile Flying and Beyond</strong>
                        </h5>
                        Yuhao Huang, Yicheng Zeng, Xiaobin Xiong
                        <br>
                        <strong>Abstract:</strong>
                            State-of-the-art model-based control strategies have demonstrated success in enabling dynamic locomotion behaviors such as flying, 
                            hopping, and walking in robotic systems. However, the performance of these behaviors in practice remains inadequate, 
                            particularly due to the inherent discrepancies between the modeled dynamics and the physical hardware, 
                            which inevitably lead to trajectory tracking errors. To mitigate this issue, we propose a semi-implicit control framework 
                            that bridges the model-to-real gap by incorporating a data-driven control approach combined with the existing model-based design. 
                            We validate the proposed method on PogoX, a custom-designed multi-modal locomotion robot, 
                            demonstrating high-precision hopping and flying behaviors in both simulation and real-world experiments. 
                            This semi-implicit control paradigm offers a generalizable solution
                            for improving performance across a broad range of robotic platforms and locomotion behaviors.
                        <br>
                        <a href="../attend/papers/11_Semi_Implicit_Data_Driven_P.pdf">[paper link]</a>
                    </div>

                    <div style="margin-top: 1em;">
                        <h5>
                            <strong>Temporal Action Representation Learning for Aerial Maneuvering and Resource-Aware Decision-Making</strong>
                            
                        </h5>
                        Jung Hoseong, Sungil Son, Daesol Cho, Jonghae Park, Changhyun Choi, H. Jin Kim
                        <br>
                            <strong>Abstract:</strong>
                                A fully autonomous agent should reason about how to deploy limited resources effectively in dynamic and uncertain environments. 
                                Despite the focus on learning to act under such constraints, the tactical use of resources in fast-evolving scenarios (e.g., air combat) 
                                remains underexplored. Addressing this challenge requires modeling how resource usage unfolds over time and influences future behavior. 
                                To this end, we explore self-supervised learning approaches tailored to modeling the causal dependency and temporal relationship between 
                                these interrelated processes. Specifically, we introduce TART, a Temporal Action contrastive learning approach that facilitates semantic 
                                alignment between Resource control and Tactical maneuvers. TART learns via contrastive learning based on a mutual information objective, 
                                carefully designed to account for both forward and backward dependencies embedded within such dynamics. These learned representations 
                                are quantized into discrete codebook entries that serve as inputs to the policy, enabling us to model the multiple tactical modes in 
                                downstream tasks. To empirically assess our method, we present an air combat simulation environment where tactical resource allocation 
                                is essential for mission success, using customized scenarios of varying difficulty to compare against baseline approaches. 
                                Extensive experiments demonstrate the effectiveness of TART in the use of limited resources and superior performance in generating tactical maneuvers.
                        <br>
                        <a href="../attend/papers/8_Temporal_Action_Representati.pdf">[paper link]</a>
                    </div>


                    <div style="margin-top: 1em;">
                        <h5>
                            <strong>AeroGrid100: A Real-World Multi-Pose Aerial Dataset for Implicit Neural Scene Reconstruction</strong>
                            
                        </h5>
                        Qingyang Zeng, Adyasha Mohanty
                        <br>
                        <strong>Abstract:</strong>
                            Aerial scene reconstruction, view synthesis, and robotics applications increasingly demand large-scale drone datasets 
                            with comprehensive spatial-angular coverage to enable robust neural rendering and embodied intelligence systems. 
                            However, existing UAV datasets typically provide only nadir views and lack precise pose annotations, 
                            limiting their utility for advanced multi-view learning tasks and requiring computationally expensive Structure-from-Motion 
                            pipelines to obtain camera poses. 
                            To address these limitations, we introduce AeroGrid100, 
                            a novel multi-pose aerial image dataset tailored for neural scene reconstruction, view synthesis, and aerial robotics research. 
                            Captured using a high-resolution DJI Air 3 drone across diverse semi-urban environments, 
                            the dataset contains over 15,000 images from 100 geospatial anchors, 
                            each systematically sampled at five altitudes and across a grid of yaw and pitch angles. 
                            AeroGrid100 provides ground-truth 6-DoF camera poses, eliminating the need for Structure-from-Motion pipelines. 
                            The dataset's dense spatial-angular coverage can enable rapid progress on research topics such as reinforcement learning-based 
                            path planning in discrete orientation spaces and geometry-aware vision-language modeling.
                        <br>
                        <a href="../attend/papers/7_AeroGrid100_A_Real_World_Mul">[paper link]</a> 
                    </div>



                    <div style="margin-top: 1em;">
                        <h5>
                            <strong>Learning to Navigate: Fully Onboard, End-to-End Navigation on Resource Constrained Quadrotor Swarms</strong>
                            
                        </h5>
                        Darren Chiu, Zhehui Huang, Ruohai Ge, Gaurav S. Sukhatme
                        <br>
                        <strong>Abstract:</strong>
                            Nano-scale unmanned aerial vehicles (UAVs) offer unprecedented agility for exploring complex environments, 
                            especially when deployed in teams. Yet their severely constrained onboard sensing, communication, 
                            and computation pose significant challenges for navigation. 
                            Most existing approaches rely on high-resolution vision or compute-intensive motion planners, 
                            rendering them infeasible for such platforms. We introduce a lightweight, 
                            safety-guided reinforcement learning (RL) framework tailored for multi-UAV navigation in cluttered spaces. 
                            Our system combines multiple low-resolution time-of-flight (ToF) depth sensors for perception with a simple motion planner and compact, 
                            attention-based deep RL policy. In simulation, our method matches the performance of two state-of-the-art baselines while using substantially fewer resources. 
                            We further demonstrate its viability on six Crazyflie quadrotors, executing fully onboard localization, perception, planning, and control.
                        <br>
                        <a href="../attend/papers/6_Learning_to_Navigate_Fully_O.pdf">[paper link]</a> 
                    </div>



                    <div style="margin-top: 1em;">
                        <h5>
                            <strong>Implicit Constraint-Aware Off-Policy Correction for Offline Reinforcement Learning</strong>
                            
                        </h5>
                        Ali Baheri
                        <br>
                        <strong>Abstract:</strong>
                            Offline reinforcement learning promises policy improvement from logged interaction data alone, 
                            yet state-of-the-art algorithms remain vulnerable to value over-estimation and to violations of domain knowledge such as 
                            monotonicity or smoothness. We introduce implicit constraint-aware off-policy correction, 
                            a framework that embeds structural priors directly inside every Bellman update. 
                            The key idea is to compose the optimal Bellman operator with a proximal projection on a convex constraint set, 
                            which produces a new operator that (i) remains a 
                            &gamma;-contraction, (ii) possesses a unique fixed point, and (iii) enforces the prescribed structure exactly. 
                            A differentiable optimization layer solves the projection; implicit differentiation supplies gradients for deep function 
                            approximators at a cost comparable to implicit Q-learning. On a synthetic Bid-Click auction—where the true value is 
                            provably monotone in the bid—our method eliminates all monotonicity violations and outperforms conservative 
                            Q-learning and implicit Q-learning in return, regret, and sample efficiency.                
                        <br>
                    </div>

                </div>
            </div>
            
            <br>
            <div class = "row">	
                <div class="col-md-12">
                    <h3 class="subtitle">Author Instructions</h3>	
                    <hr class="m-y-2">
                    <li>Final full paper submission: Please follow the instructions sent by email.</li>
                    <li>Oral presentation: There will be a 1~2 minute talk for each accepted paper before the coffee break. Please prepare a one-slide introduction for your paper.</li>
                    <li>Poster session: Please use the time before the workshop starts to set-up the posters. 
                        The poster session will be held during the coffee break from 09:40 - 10:20 am.

                        
                        Workshop posters will follow the same size as those used for the conference papers
                        <a href="https://roboticsconference.org/information/presentationInstructions/" target="_blank"
                        style="text-decoration: none; color: inherit;"
                        onmouseover="this.style.color='#007bff';"
                        onmouseout="this.style.color='inherit';">
                        <strong>as described here</strong>
                        </a>.

                    </li>
                </div>
            </div> 

            <br>  
            <div class = "row">	     
                <div class="col-md-12">
                    <h3 class="subtitle"> Online Participation</h3>	
                    <hr class="m-y-2">	
               
                    Virtual attendees will have access to all sessions, 
                    including live-streamed talks, discussions, and Q&A sessions, 
                    ensuring that remote participation is interactive and engaging. 
                </div>
            </div>

    </section>


</body>